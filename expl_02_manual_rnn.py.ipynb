{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## hyper-parameters\n",
    "##\n",
    "alpha = 0.1\n",
    "nn_input_dim = 1\n",
    "nn_recurrent_layer_neurons = 2\n",
    "nn_output_neurons = 1\n",
    "nn_recurrent_deepness = 2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "np.random.random((nn_input_dim,nn_recurrent_layer_neurons)) - 1\n",
    "\n",
    "with tf.name_scope('W_input'):\n",
    "    W_input = tf.Variable(tf.random_normal( \\\n",
    "        shape=[nn_input_dim, nn_recurrent_layer_neurons], \\\n",
    "        dtype=tf.float32), name='W_input')\n",
    "\n",
    "with tf.name_scope('b_input'):\n",
    "    b_input = tf.Variable(1., name='b_input')\n",
    "    \n",
    "with tf.name_scope('W_recurrent'):\n",
    "    W_recurrent = tf.Variable(tf.random_normal( \\\n",
    "        shape=[nn_recurrent_layer_neurons, nn_recurrent_layer_neurons], \\\n",
    "        dtype=tf.float32), name='W_recurrent')\n",
    "\n",
    "with tf.name_scope('b_recurrent'):\n",
    "    b_input = tf.Variable(1., name='b_recurrent')\n",
    "\n",
    "with tf.name_scope('W_output'):    \n",
    "    W_output = tf.Variable(tf.random_normal(\\\n",
    "        shape=[nn_recurrent_layer_neurons, nn_output_neurons], \\\n",
    "        dtype=tf.float32), name='W_output')\n",
    "\n",
    "with tf.name_scope('b_output'):\n",
    "    b_output = tf.Variable(1., name='b_output')\n",
    "\n",
    "with tf.name_scope('Y_train'):\n",
    "    Y_train = tf.placeholder(tf.float32, name='Y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## re-current input matrix multiply (inputs: X_train, h_recurrent)\n",
    "g_recursion = 0\n",
    "\n",
    "rnn_op = {}\n",
    "rnn_op_index = 0\n",
    "\n",
    "X_input = {}\n",
    "x_input_index = 0\n",
    "\n",
    "Y_output = {}\n",
    "y_output_index = 0\n",
    "\n",
    "def inc_rnn_op_index():\n",
    "    global rnn_op_index\n",
    "    rnn_op_index = rnn_op_index +1\n",
    "    return rnn_op_index - 1\n",
    "\n",
    "def inc_x_input_index():\n",
    "    global x_input_index\n",
    "    x_input_index = x_input_index + 1\n",
    "    return x_input_index - 1\n",
    "\n",
    "def inc_y_output_op_index():\n",
    "    global y_output_index\n",
    "    y_output_index = y_output_index + 1\n",
    "    return y_output_index - 1\n",
    "\n",
    "def inc_g_recursion():\n",
    "    global g_recursion\n",
    "    g_recursion = g_recursion + 1\n",
    "    return g_recursion -1\n",
    "    \n",
    "def g_RNN(prev_y_output_recursion_index=0):\n",
    "    \n",
    "    global X_input\n",
    "    global Y_output_op\n",
    "    global rnn_op\n",
    "    \n",
    "    x_input_index_tmp = inc_x_input_index()\n",
    "    y_output_op_index_tmp = inc_y_output_op_index()\n",
    "    g_recursion = inc_g_recursion()\n",
    "    \n",
    "    # define placeholder for input\n",
    "    with tf.name_scope('rnn_input' + str(g_recursion)):\n",
    "           \n",
    "        X_input[x_input_index_tmp] = tf.placeholder(tf.float32, \\\n",
    "               [None, nn_input_dim], name='X_input' + str(x_input_index_tmp))\n",
    "        print('[d] placeholder: X_input' + str(x_input_index_tmp), ' created.') \n",
    "        \n",
    "    # define nested operations \n",
    "    with tf.name_scope('rnn' + str(g_recursion)):\n",
    "        \n",
    "        print('[d] creating operations for node: rnn' + str(g_recursion))\n",
    "        \n",
    "        # matmul X * W @t==0\n",
    "        y_output_a_index = inc_rnn_op_index()\n",
    "        if prev_y_output_recursion_index == False:\n",
    "            # bottom of rnn, no linked node, just multiply XW\n",
    "            rnn_op[y_output_a_index] = tf.matmul(X_input[x_input_index_tmp], \\\n",
    "                  W_input)\n",
    "        else:\n",
    "            # add result of previous linked node output, multiply XW + previous Y\n",
    "            rnn_op[y_output_a_index] = tf.add(tf.matmul(X_input[x_input_index_tmp], \\\n",
    "                  W_input), rnn_op[prev_y_output_recursion_index])\n",
    "    \n",
    "        # add bias to computation result\n",
    "        y_output_b_index = inc_rnn_op_index()\n",
    "        rnn_op[y_output_b_index] = tf.add(rnn_op[y_output_a_index], b_input)\n",
    "    \n",
    "        # activation function is the last ouput operation of rnn node - Y_out\n",
    "        y_output_c_index = inc_rnn_op_index()\n",
    "        rnn_op[y_output_c_index] = tf.sigmoid(rnn_op[y_output_b_index], name='Y_out' + str(g_recursion))            \n",
    "        \n",
    "        print('[d] Y_output operation for node: rnn' + \\\n",
    "              str(g_recursion) + ' is rnn_op[', str(y_output_c_index), ']')\n",
    "\n",
    "        # precompute matmul of Y*W_recurrent to reuse with up-nested layer\n",
    "        y_output_recursion_index = inc_rnn_op_index()\n",
    "        rnn_op[y_output_recursion_index] = tf.matmul(rnn_op[y_output_c_index], \\\n",
    "              W_recurrent, name='Y_out_rec' + str(g_recursion))\n",
    "        \n",
    "        print('[d] Y_out_rec operation for node: rnn' + \\\n",
    "              str(g_recursion) + ' is rnn_op[', str(y_output_recursion_index), ']')\n",
    "\n",
    "    return y_output_recursion_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## data\n",
    "##\n",
    "feed_X_train = np.array([[0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0]])\n",
    "feed_Y_train = np.array([['x'], [1.0], ['x'], [1.0], ['x'], [0.0], ['x'], [0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "def test():\n",
    "    \n",
    "    print()\n",
    "    for pointer in range(0, len(feed_X_train), 2):\n",
    "        pred_res = sess.run([pred], feed_dict={\\\n",
    "                            X_input[0]: feed_X_train[pointer]   [None, :], \\\n",
    "                            X_input[1]: feed_X_train[pointer+1] [None, :]})\n",
    "    \n",
    "        print(feed_X_train[pointer], feed_X_train[pointer+1], np.around(pred_res))\n",
    "\n",
    "    print()\n",
    "    for pointer in range(0, len(feed_X_train), 2):\n",
    "        pred_res = sess.run([pred], feed_dict={\\\n",
    "                            X_input[0]: feed_X_train[pointer]   [None, :], \\\n",
    "                            X_input[1]: feed_X_train[pointer+1] [None, :]})\n",
    "\n",
    "        print(feed_X_train[pointer], feed_X_train[pointer+1], pred_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[d] placeholder: X_input0  created.\n",
      "[d] creating operations for node: rnn0\n",
      "[d] Y_output operation for node: rnn0 is rnn_op[ 2 ]\n",
      "[d] Y_out_rec operation for node: rnn0 is rnn_op[ 3 ]\n",
      "[d] placeholder: X_input1  created.\n",
      "[d] creating operations for node: rnn1\n",
      "[d] Y_output operation for node: rnn1 is rnn_op[ 6 ]\n",
      "[d] Y_out_rec operation for node: rnn1 is rnn_op[ 7 ]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Session\n",
    "##\n",
    "with tf.name_scope('computation'):\n",
    "    # graph definition\n",
    "    rnn_prediction_chain = g_RNN(g_RNN())\n",
    "    # prediction from RNN into single neuron\n",
    "    pred = tf.sigmoid(tf.add(tf.matmul(rnn_op[rnn_prediction_chain], W_output), b_output))\n",
    "    # cost\n",
    "    cost_function = tf.reduce_sum(tf.subtract(pred, Y_train) ** 2)\n",
    "    # optimizer\n",
    "    learning_rate = 0.25\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost_function)\n",
    "    #optimizer = tf.train.AdagradOptimizer(learning_rate=0.1).minimize(cost_function)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "    #optimizer = tf.train.MomentumOptimizer(0.2, 0.8).minimize(cost_function)\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "tb_writer = tf.summary.FileWriter(\"./tensorboard/\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.] [ 1.]  Xor  ['1.0']\n",
      "[ 1.] [ 0.]  Xor  ['1.0']\n",
      "[ 0.] [ 0.]  Xor  ['0.0']\n",
      "[ 1.] [ 1.]  Xor  ['0.0']\n",
      "[i] train batches: \n",
      "X0:  [array([ 0.]), array([ 1.]), array([ 0.]), array([ 1.])]\n",
      "X1:  [array([ 1.]), array([ 0.]), array([ 0.]), array([ 1.])]\n",
      "Y:  [array(['1.0'], \n",
      "      dtype='<U3'), array(['1.0'], \n",
      "      dtype='<U3'), array(['0.0'], \n",
      "      dtype='<U3'), array(['0.0'], \n",
      "      dtype='<U3')]\n",
      "[i] training\n",
      "epoch:  0  cost:  1.84884\n",
      "epoch:  1000  cost:  0.245729\n",
      "epoch:  2000  cost:  0.00474982\n",
      "[i] training finished, cost:  0.000998753\n"
     ]
    }
   ],
   "source": [
    "zip_x = {}\n",
    "zip_y = {}\n",
    "\n",
    "train_batch_X0 = []\n",
    "train_batch_X1 = []\n",
    "train_batch_Y = []\n",
    "\n",
    "# print logical function    \n",
    "for pointer in range(0, len(feed_X_train), 2):\n",
    "    print(feed_X_train[pointer], feed_X_train[pointer+1], \\\n",
    "          ' Xor ', feed_Y_train[pointer+1])\n",
    "    train_batch_X0.append(feed_X_train[pointer])\n",
    "    train_batch_X1.append(feed_X_train[pointer+1])\n",
    "    train_batch_Y.append(feed_Y_train[pointer+1])\n",
    "    \n",
    "    \n",
    "print('[i] train batches: ')\n",
    "print('X0: ', train_batch_X0)\n",
    "print('X1: ', train_batch_X1)\n",
    "print('Y: ',  train_batch_Y)\n",
    "\n",
    "epoch = -1\n",
    "cost_function_res = 1000\n",
    "\n",
    "print('[i] training')\n",
    "while cost_function_res > 0.001:\n",
    "#for i in range(10000):\n",
    "    epoch = epoch + 1\n",
    "    for pointer in range(0, len(feed_X_train), 2):\n",
    "        \n",
    "        optimizer_res, cost_function_res = sess.run([optimizer, cost_function], \\\n",
    "            feed_dict={X_input[0]: [[0], [0], [1], [1]], \\\n",
    "                       X_input[1]: [[0], [1], [0], [1]], \\\n",
    "                       Y_train:    [[0], [1], [1], [0]]})\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print('epoch: ', epoch, ' cost: ', cost_function_res)\n",
    "        \n",
    "    if epoch > 100000:\n",
    "        break\n",
    "\n",
    "print('[i] training finished, cost: ', cost_function_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
